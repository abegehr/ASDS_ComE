\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{listings}
\lstset{
  columns=fullflexible, 
  frame=single,
  breaklines=true,
}

\AtBeginDocument{
  \providecommand\BibTeX{{
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{none}
\copyrightyear{}
\acmDOI{}
\acmISBN{}
\acmConference[Data Science Seminar]{Data Science Seminar}{2019}{Uni Passau}


\begin{document}

\title{[Experiment] Learning Community Embedding with Community Detection and Node Embedding on Graph (CD)}

\author{Anton Begehr}
\affiliation{
  \institution{University of Passau}
  \streetaddress{Innstra√üe 33}
  \city{Passau}
  \country{Germany}}
\email{a.begehr@fu-berlin.de}


\begin{abstract}
  The graph embedding algorithm ComE developed by \citeauthor{Cav17} in their \citeyear{Cav17} paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graph} combines the powers of community detection, community embedding, and node embedding in a closed loop optimizing algorithm to generate low dimensional embeddings for communities and nodes in a graph. This research paper undertakes an experiment with the aims to explore ComE's inner workings and measure its effectivity of classifying crawled twitter data in comparison to Louvain Modularity with normalized mutual information.
\end{abstract}


\maketitle

\section{Introduction}

In their \citeyear{Cav17} paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graph} the authors \citeauthor{Cav17} explore graph embeddings by utilizing a three step closed loop optimization process consisting of Community Detection, Community Embedding, and Node Embedding: ComE.\cite{Cav17} They then apply ComE and further graph embedding algorithms on excerpts of multiple, well known graph datasets: Karate Club, BlogCatalog, Flicker, Wikipedia, DBLP. They choose DeepWalk/SF, Line, Node2Vec, GraRep, and M-NMF to measure the quality of ComE's results using Micro-F1 and Macro-F1 for classification results and conductance and normalized mutual information (NMI) for the resulting graph embeddings. It is worthwhile to note, that ComE works with elementary graphs, meaning graphs consisting of nodes and edges, disregarding, for example, node properties, edge properties, and edge weights.

In this paper we will explore the graph embedding algorithm ComE developed by \citeauthor{Cav17}, generate embeddings using ComE for the Twitter dataset \cite{TwitterData} crawled by \citeauthor{TwitterData}, and compare the results to communities gained by applying Louvain Modularity using normalized mutual information (NMI) as a comparison measure.


\section{Twitter Data} \label{twitter_data}

Crawled datasets from Twitter, combined with event data that is supplied in the public GitHub repository \textit{fatemehsrz/Twitter\_Data} will be used.\cite{TwitterData}

\subsection{Exploration}

The supplied twitter data consists of three groups. Each concerning the twitter interactions of a user either attending or not attending one of three conferences. The three conferences are encoded as \textit{anthrocon}, \textit{comiccon2017}, and \textit{icann2016}.

For each conference two CSVs are supplied:

\begin{enumerate}
	\item tweets: For each tweet an unique identifier, the content of tweet, an identifier for the user who posted the tweet and a label determining if the suer attended the event is supplied. 
	\item edgelist: Each row represents an edge between two users. It will be assumed, that each edge represents an interaction on twitter. For example commenting and liking would count as interactions.
\end{enumerate}

The tweets id and content will be disregarded in our investigation, because we assume the tweet id does not include information we can utilize and we will not be participating in any NLP practices in this experiment.

\subsection{Requirements}

Both the problem space and ComE itself have requirements that the data needs to fulfill. The problem space of graph embeddings is first and foremost classification.

Next to obvious requirements of graph embedding algorithms, the following requirements need to be fulfilled to apply ComE on the provided Twitter data specifically:

\begin{enumerate}
	\item The ComE algorithm requires the graph data to be supplied as a \textbf{sparse adjacency matrix}. The example code uses a MATLAB .mat file import. The data supplied by \citeauthor{TwitterData} is in an edge list format inside a CSV file. Each row represents an edge from one node to the other.
	\item ComE requires the nodes to be \textbf{labeled} for training to determine the number K of clusters it should optimize for, also we will be using the ground truth to test the resulting clusters against.
\end{enumerate}

\subsection{Data Preparation}

The two requirements of formatting the graph as a sparse adjacency matrix and the nodes with labels that make sense from an application standpoint will be fulfilled with data preprocessing in the existing ComE infrastructure and outside of it utilizing Jupyter Notebooks. These steps are needed to be able to effectively apply the ComE algorithm on the twitter data for community embeddings.

\subsubsection{Sparse Adjacency Matrix}
ComE takes a sparse adjacency matrix as input for representing the graph on which to apply the algorithm.

The ComE repository includes functionality in \textit{graph\_utils.py} for importing the graph as a sparse adjacency matrix from a MatLib file.\cite{ComE} The graph obtained from crawling twitter data is in the format of an edge list with one edge per row of the csv file. To convert an edge list to a sparse adjacency matrix, two functions were added to \textit{graph\_utils.py}:

\begin{itemize}
	\item $adjacency\_matrix\_from\_edges()$
	\item $load\_csv\_edges()$
\end{itemize}

See section \ref{edge_lists} for details on the implementation.

\subsubsection{Labels} ComE requires a labeled training dataset to be able to execute community embeddings. Specifically, ComE determines the number of clusters $K$ from the labels \cite{ComE} and we will be using the ground truth supplied by the labels to test ComE's resulting communities against.

Jupyter notebooks have been chosen for this task, due to the ability to swiftly iterate on solutions and algorithms, also the size of the supplied data allows for local computation on a personal computer.

The preprocessing procedure for the twitter data mentioned above allows us to utilize ComE's ability to cluster nodes purely from interaction data and then test the results against our ground truth. The ground truth being if a specific user attended an event or not.

\subsection{Labels}

The users of each event dataset correspond to the nodes of the graph. Considering we have three datasets with a user identification column, this raises the following question: Do user identifiers apply across events? This question is important to know if the event datasets can be unioned and create an analysis on the whole dataset instead of doing multiple separate analyses.

If the user identifier applies over all three events, the data can be unioned and eight labels (communities) could be extrapolated:

\begin{enumerate}
	\item attended no conferences
	\item attended anthrocon
	\item attended comiccon2017
	\item attended icann2016
	\item attended anthrocon, and comiccon2017
	\item attended anthrocon, and icann2016
	\item attended comiccon2017, and icann2016
	\item attended anthrocon, comiccon2017, and icann2016
\end{enumerate}

These eight labels combining the data of all three events seemed logical at first and was implemented in the jupyter notebook at \textit{preprocessing/generate\_labels-old.ipynb}.

Further data exploration led to the insight, that the user identifiers for all datasets start with the value $0$ and all user identifiers between the value $0$ and the maximum of the user identifiers for that event are present in the specific event. See \textit{preprocessing/user\_ids.ipynb} for detailed insights. Both insights suggest, that the user identifier does not after all apply across event datasets. Therefore, the event datasets will need to be tested individually. For each dataset there will be a separate binary labeling:

\begin{enumerate}
	\item did attended conference
	\item did not attended conference
\end{enumerate}

The implementation for binary label generation can be found here: \textit{preprocessing/generate\_labels.ipynb}

\section{Experiment}

The experiment was run on the twitter data described in Section \ref{twitter_data} using Python 3.6 and PyCharm. All code that has been used and written for the experiments is available at the GitHub repository \textit{abegehr/}

\subsection{Hyper-parameters}

ComE has multiple hyper-parameters.

For simplicity reasons, the hyper-parameters were not tuned. The experiment uses the default hyper-parameters that are present in the ComE GitHub repository for running an experiment on the DBLP dataset\cite{ComE}.

\section{Results}

Running the experiment generates multiple files. One of the files generated is a text-file containing the results of the experiment. The results text-file is named \textit{twitter\_alpha-0.1\_beta-0.1\_ws-10\_neg-5\_lr-0.025\_icom-62\_ind-62\_k-5\_ds-0.0.txt}, containing references to the dataset and hyper-parameters used.

The results text-file has 2495 rows, one for each node. The first row for node 0 reads like follows:
\begin{lstlisting}
0	0.119466 -0.00167963 -0.0100419 -0.34768 -0.11141 -0.194391 -0.227025 0.265465 -0.256364 -0.144862 -0.201504 -0.259735 -0.0231989 -0.219537 -0.356864 -0.245884 -0.0975166 -0.280691 -0.0467756 0.189156 -0.0508059 0.139592 0.0410849 0.282847 0.0828746 -0.117361 -0.0487821 0.0498272 0.0363623 0.136442 -0.0659485 -0.0727705 0.23538 0.0174028 -0.151602 -0.268329 0.0175012 -0.240817 -0.159485 0.0954141 -0.0240037 -0.223511 -0.0585588 -0.0422711 0.0720152 -0.0403358 0.0575148 -0.0742481 -0.260205 0.018126 0.104849 -0.155449 -0.12444 0.222539 0.102688 -0.271191 -0.0153477 -0.00814301 -0.0245261 -0.0373094 -0.170265 0.0142297 0.247277 -0.249435 0.0778326 0.247973 -0.154441 0.351208 0.0586203 -0.0348056 0.0716461 -0.0177695 0.164301 -0.193287 -0.0499279 -0.0730012 -0.078206 -0.0191653 0.130336 0.233464 0.075438 0.0782676 -0.312208 0.260226 -0.0272545 -0.13308 -0.0962206 -0.294585 -0.0398387 0.196411 0.0928808 0.00576813 -0.00238099 -0.0967206 -0.170343 0.13645 0.377029 0.133433 -0.269607 0.0328925 0.252237 -0.222757 0.136386 0.118541 -0.0201975 -0.112246 -0.194241 -0.291803 0.102886 -0.0241 -0.244125 0.146849 0.238158 -0.17695 0.0589843 -0.10204 -0.0934185 0.0738336 0.0714247 -0.137283 0.380721 -0.34169 0.432387 0.239321 -0.0428069 0.263721 -0.108774 -0.0988426`
\end{lstlisting}

\subsection{Louvain Modularity}

The results from applying Louvain Modularity will be presented and briefly discussed.

\section{Comparison}

The results from applying ComE and Louvain Modularity will be compared utilizing NMI.

\section{Graph Visualizations}

The goal is to visualize the results in a two dimensional space.

\section{Critique}

Critique of ComE.

For example the number of communities K needs to be supplied for the algorithm to work.

\section{Algorithm}

Deep dive into the technical workings of the algorithm.

For example: Gaussian Mixture Model for community embeddings and likelihood maximization.

\section{Conclusion}

Why is ComE better or worse at this specific task?


\bibliographystyle{ACM-Reference-Format}
\bibliography{sources}


\begin{acks}
To the reader of this early draft. Please mark which sections of the paper, you would find the most interesting and worthwhile to go deep on.
\end{acks}


\appendix

\section{Code}

\subsection{Generate graph}

\begin{lstlisting}[language=python]
import numpy as np
# union all edge lists into one graph

# load data
path = "data/graphs/"
edges1 = np.genfromtxt(path+"anthrocon_edgelist.csv", dtype=int, delimiter=',')
edges2 = np.genfromtxt(path+"comiccon2017_edgelist.csv", dtype=int, delimiter=',')
edges3 = np.genfromtxt(path+"icann2016_edgelist.csv", dtype=int, delimiter=',')

# union edges
graph = np.concatenate([edges1, edges2, edges3])
graph.shape

np.savetxt('twitter.csv', graph, delimiter=',', fmt=['%i', '%i'])
\end{lstlisting}

\subsection{Generate Labels}

\begin{lstlisting}[language=python]
import pandas as pd
# generate labels from tweet data

# load data
path = "data/tweets/"
tweets1 = pd.read_csv(path+"anthrocon_tweets.csv", delimiter=',')
tweets2 = pd.read_csv(path+"comiccon2017_tweets.csv", delimiter=',')
tweets3 = pd.read_csv(path+"icann2016_tweets.csv", delimiter=',')

def get_attendees(tweets):
    return tweets.loc[tweets['attendance_label'] == 1]['node_id']

nodes1 = get_attendees(tweets1).to_numpy()
nodes2 = get_attendees(tweets2).to_numpy()
nodes3 = get_attendees(tweets3).to_numpy()

all_nodes = np.concatenate([nodes1, nodes2, nodes3])

node_min = all_nodes.min()
node_max = all_nodes.max()
print("node_min: ", node_min)
print("node_max: ", node_max)

nodes = []
for i in range(node_min, node_max + 1):
    a = (i in nodes1, i in nodes2, i in nodes3)
    
    if a == (1, 1, 1): label = 7
    elif a == (0, 1, 1): label = 6
    elif a == (1, 0, 1): label = 5
    elif a == (1, 1, 0): label = 4
    elif a == (0, 0, 1): label = 3
    elif a == (0, 1, 0): label = 2
    elif a == (1, 0, 0): label = 1
    else: label = 0
    
    nodes.append([i, label])

np.savetxt('twitter.labels', nodes, delimiter='\t', fmt=['%i', '%i'])
\end{lstlisting}

\subsection{Read CSVs} \label{edge_lists}

\begin{lstlisting}[language=python]
import numpy as np
from scipy.sparse import coo_matrix

def adjacency_matrix_from_edges(edges):
    return coo_matrix((np.zeros(len(edges)), (edges[:, 0], edges[:, 1])))

def load_csv_edges(file_, undirected=True):
  edges = np.genfromtxt(file_, dtype=int, delimiter=',')

  adj_matrix = adjacency_matrix_from_edges(edges)

  return from_numpy(adj_matrix, undirected)
\end{lstlisting}


\end{document}
\endinput
