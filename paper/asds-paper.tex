\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{listings}
\lstset{
  columns=fullflexible, 
  frame=single,
  breaklines=true,
}

\AtBeginDocument{
  \providecommand\BibTeX{{
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{none}
\copyrightyear{}
\acmDOI{}
\acmISBN{}
\acmConference[Data Science Seminar]{Data Science Seminar}{2019}{Uni Passau}


\begin{document}

\title{[Experiment] Learning Community Embedding with Community Detection and Node Embedding on Graph (CD)}

\author{Anton Begehr}
\affiliation{
  \institution{University of Passau}
  \streetaddress{Innstra√üe 33}
  \city{Passau}
  \country{Germany}}
\email{a.begehr@fu-berlin.de}


\begin{abstract}
  The graph embedding algorithm ComE developed by \citeauthor{Cav17} in their \citeyear{Cav17} paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graph} combines the powers of community detection, community embedding, and node embedding in a closed loop optimizing algorithm to generate low dimensional embeddings for communities and nodes in a graph. This research paper undertakes an experiment with the aims to explore ComE's inner workings and measure its effectivity of classifying crawled twitter data in comparison to Louvain Modularity with normalized mutual information.
\end{abstract}


\maketitle

\section{Introduction}

In their \citeyear{Cav17} paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graph} the authors \citeauthor{Cav17} explore graph embeddings by utilizing a three step closed loop optimization process consisting of Community Detection, Community Embedding, and Node Embedding: ComE.\cite{Cav17} They then apply ComE and further graph embedding algorithms on excerpts of multiple, well known graph datasets: Karate Club, BlogCatalog, Flicker, Wikipedia, DBLP. They choose DeepWalk/SF, Line, Node2Vec, GraRep, and M-NMF to measure the quality of ComE's results using Micro-F1 and Macro-F1 for classification results and conductance and normalized mutual information (NMI) for the resulting graph embeddings. It is worthwhile to note, that ComE works with elementary graphs, meaning graphs consisting of nodes and edges, disregarding, for example, node properties, edge properties, and edge weights.

In this paper we will explore the graph embedding algorithm ComE developed by \citeauthor{Cav17}, generate embeddings using ComE for the Twitter dataset \cite{TwitterData} crawled by \citeauthor{TwitterData}, and compare the results to communities gained by applying Louvain Modularity using normalized mutual information (NMI) as a comparison measure.


\section{Twitter Data} \label{twitter_data}

Crawled datasets from Twitter, combined with conference data that is supplied in the public GitHub repository \textit{fatemehsrz/Twitter\_Data} will be used.\cite{TwitterData}

\subsection{Exploration}

The supplied twitter data consists of three groups. Each concerning the twitter interactions of a user either attending or not attending one of three conferences. The three conferences are labeled as (1) \textit{anthrocon}, (2) \textit{comiccon2017}, and (3) \textit{icann2016}.

For each conference two CSVs are supplied:

\begin{enumerate}
	\item tweets: For each tweet an unique identifier, the content of tweet, an identifier for the user who posted the tweet and a label determining if the suer attended the conference is supplied. 
	\item edgelist: Each row represents an edge between two users. It will be assumed, that each edge represents an interaction on twitter. For example commenting and liking would count as interactions.
\end{enumerate}

The tweets id and content will be disregarded in our investigation, because we assume the tweet id does not include information we can utilize since ComE does not support property graphs and we will not be participating in any NLP practices in this experiment.

The tweet and edges datasets of the conferences have the following metrics:

\begin{enumerate}
	\item \textit{anthrocon}
	\begin{itemize}
		\item tweets: $3842$
		\item users: $1060$
		\item edges: $15788$
		\item average edges per user: $\approx14.9$
	\end{itemize}
	\item \textit{comiccon2017}
	\begin{itemize}
		\item tweets: $4381$
		\item users: $2495$
		\item edges: $8203$
		\item average edges per user: $\approx3.3$
	\end{itemize}
	\item \textit{icann2016}
	\begin{itemize}
		\item tweets: $2550$
		\item users: $1016$
		\item edges: $17045$
		\item average edges per user: $\approx16.8$
	\end{itemize}
\end{enumerate}

Find the jupyter notebook used for these calculations at \textit{preprocessing/exploratory.ipynb}.

\subsection{Requirements}

Both the problem space and ComE itself have requirements that the data needs to fulfill. The problem space of graph embeddings is first and foremost classification.

Next to obvious requirements of graph embedding algorithms, the following requirements need to be fulfilled to apply ComE on the provided Twitter data specifically:

\begin{enumerate}
	\item The ComE algorithm requires the graph data to be supplied as a \textbf{sparse adjacency matrix}. The example code uses a MATLAB .mat file import. The data supplied by \citeauthor{TwitterData} is in an edge list format inside a CSV file. Each row represents an edge from one node to the other.
	\item ComE requires the nodes to be \textbf{labeled} for training to determine the number K of clusters it should optimize for, also we will be using the ground truth to test the resulting clusters against.
\end{enumerate}

\subsection{Data Preparation}

The two requirements of formatting the graph as a sparse adjacency matrix and the nodes with labels that make sense from an application standpoint will be fulfilled with data preprocessing in the existing ComE infrastructure and outside of it utilizing Jupyter Notebooks. These steps are needed to be able to effectively apply the ComE algorithm on the twitter data for community embeddings.

\subsubsection{Sparse Adjacency Matrix}
ComE takes a sparse adjacency matrix as input for representing the graph on which to apply the algorithm.

The ComE repository includes functionality in \textit{graph\_utils.py} for importing the graph as a sparse adjacency matrix from a MatLib file.\cite{ComE} The graph obtained from crawling twitter data is in the format of an edge list with one edge per row of the csv file. To convert an edge list to a sparse adjacency matrix, two functions were added to \textit{graph\_utils.py}:

\begin{itemize}
	\item $adjacency\_matrix\_from\_edges()$
	\item $load\_csv\_edges()$
\end{itemize}

See section \ref{edge_lists} for details on the implementation.

\subsubsection{Labels} ComE requires a labeled training dataset to be able to execute community embeddings. Specifically, ComE determines the number of clusters $K$ from the labels \cite{ComE} and we will be using the ground truth supplied by the labels to test ComE's resulting communities against.

Jupyter notebooks have been chosen for this task, due to the ability to swiftly iterate on solutions and algorithms, also the size of the supplied data allows for local computation on a personal computer.

The preprocessing procedure for the twitter data mentioned above allows us to utilize ComE's ability to cluster nodes purely from interaction data and then test the results against our ground truth. The ground truth being if a specific user attended an conference or not.

\subsection{Labels}

The users of each conference dataset correspond to the nodes of the graph. Considering we have three datasets with a user identification column, this raises the following question: Do user identifiers apply across conferences? This question is important to know if the conference datasets can be unioned and create an analysis on the whole dataset instead of doing multiple separate analyses.

If the user identifier applies over all three conferences, the data can be unioned and eight labels (communities) could be extrapolated:

\begin{enumerate}
	\item attended no conferences
	\item attended anthrocon
	\item attended comiccon2017
	\item attended icann2016
	\item attended anthrocon, and comiccon2017
	\item attended anthrocon, and icann2016
	\item attended comiccon2017, and icann2016
	\item attended anthrocon, comiccon2017, and icann2016
\end{enumerate}

These eight labels combining the data of all three conferences seemed logical at first and was implemented in the jupyter notebook at \textit{preprocessing/generate\_labels-old.ipynb}.

Further data exploration led to the insight, that the user identifiers for all datasets start with the value $0$ and all user identifiers between the value $0$ and the maximum of the user identifiers for that conference are present in the specific conference. See \textit{preprocessing/user\_ids.ipynb} for detailed insights. Both insights suggest, that the user identifier does not after all apply across conference datasets. Therefore, the conference datasets will need to be tested individually. For each dataset there will be a separate binary labeling:

\begin{enumerate}
	\item did attended conference
	\item did not attended conference
\end{enumerate}

The implementation for binary label generation can be found here: \textit{preprocessing/generate\_labels.ipynb}

\section{Experiment}

The experiment was run on the twitter data described in Section \ref{twitter_data} using Python 3.6 and PyCharm 2019.1.4 (Professional Edition). All code that has been used and is written to run this experiment is available at the GitHub repository \textit{abegehr/ASDS\_ComE}:\\\url{https://github.com/abegehr/ASDS_ComE}.\cite{asds}

\subsection{Hyperparameters}

ComE requires setting multiple hyperparameters. Starting with the number $K$ of communities to be identified, over the size of batches to be used for computation, through to the number of iterations the algorithm should run through, many hyperparameters need to be set.
The experiment uses the following hyperparameters:

\begin{lstlisting}[language=python]
# hyperparameters
number_walks = 10
walk_length = 80
representation_size = 2
num_workers = 10
num_iter = 3
reg_covar = 0.00001
batch_size = 50
window_size = 10
negative = 5
lr = 0.025
alpha_betas = [(0.1, 0.1)]
down_sampling = 0.0
ks = [2]
\end{lstlisting}

Find following a brief description of each hyperparameter:\cite{Cav17}
\begin{itemize}
	\item \textit{number\_walks}: number of random walks for each node in sampling
	\item \textit{walk\_length}: length of each random walk path in sampling
	\item \textit{representation\_size}: number of dimensions of embedding space
	\item \textit{num\_workers}: number of threads to use for computation
	\item \textit{num\_iter}: number of overall iterations to run
	\item \textit{reg\_covar}: regularization coefficient to ensure positive covariance
	\item \textit{batch\_size}: number of nodes in batch
	\item \textit{window\_size}: windows size used to compute the context embedding
	\item \textit{negative}: number of negative samples
	\item \textit{lr}: learning rate
	\item \textit{alpha\_betas}: arrays of combinations of alpha and beta
	\begin{itemize}
    	\item \textit{alpha}: Trade-off parameter for context embedding
    	\item \textit{beta}: Trade-off parameter for community embedding
    \end{itemize}
	\item \textit{down\_sampling}: relative amount to reduce the sample by
	\item \textit{ks}: array of number of communities
\end{itemize}

The hyperparameters have been adjusted to follow advice from the original paper by \citet{Cav17}. Further hyperparameter tuning has not been applied.

\subsection{Process}

The code provided at the GitHub repository \textit{abegehr/ASDS\_ComE}\cite{asds}, which includes the ComE algorithm from the GitHub repository \textit{andompesta/ComE}\cite{ComE} comes in a state where it is ready to run and results are included in the committed data.

If the input data should be altered to run the experiments on a different datasets, running the experiment requires the following steps:

\begin{enumerate}
	\item Generate labels for nodes using the Jupyter notebook \textit{preprocessing/generate\_labels.ipynb} and move to \textit{ComE/data/twitter/twitter.lables}.
	\item Copy edge list to \textit{ComE/data/twitter/twitter.csv}
	\item Open \textit{ComE/} in PyCharm. The input and output file paths in \textit{main.py} should be set correctly.
	\item Run using the BICE configuration with Python 3.6.
	\item This will generate multiple files in \textit{data/}. These are the results of the community embedding.
\end{enumerate}

It is also possible to run ComE a different set of graph data. To accomplish this, the edge list and labeled nodes dataset needs to be placed in the correctly named folder inside \textit{ComE/} and the settings and hyperparameters in \textit{ComeE/main.py} should be configured accordingly. See the original paper by \citeauthor{Cav17}\cite{Cav17} and the associated GitHub repository \textit{andompesta/ComE}\cite{ComE} for further details.

\subsection{Results}

Running the experiment generates multiple files in \textit{ComE/data/}. The following files are generated:
\begin{itemize}
	\item \textit{g\_mixture.joblib}: community embedding in the form of a saved binary of the gaussian mixture model where each gaussian represents one community.
	\item \textit{labels\_pred.txt}: predicted labels for each node in the order or nodes supplied by \textit{twitter.labels}.
	\item \textit{twitter\_alpha-0.1\_beta-5\_ws-10\_neg-5\_lr-0.025\_icom-62\_ind-62\_k-2\_ds-0.0.txt}: node embeddings for each node where each line holds the node id and one value for each of the $K$ dimension of the embedding space.
\end{itemize}

ComE was executed for all three conferences: (1) \textit{anthrocon}, (2) \textit{comiccon2017}, and (3) \textit{icann2016}. The resulting files can be found at:
\begin{enumerate}
	\item anthrocon: \textit{}
	\item comiccon2017: \textit{}
	\item icann2016: \textit{}
\end{enumerate}


\subsection{Louvain Modularity}

The results from applying Louvain Modularity will be presented and briefly discussed.

\section{Comparison}

The results from applying ComE and Louvain Modularity will be compared utilizing NMI.

\section{Graph Visualizations}

The goal is to visualize the results in a two dimensional space.

\section{Critique}

Critique of ComE.

For example the number of communities K needs to be supplied for the algorithm to work.

\section{Algorithm}

Deep dive into the technical workings of the algorithm.

For example: Gaussian Mixture Model for community embeddings and likelihood maximization.

\section{Conclusion}

Why is ComE better or worse at this specific task?


\bibliographystyle{ACM-Reference-Format}
\bibliography{sources}


\begin{acks}
To the reader of this early draft. Please mark which sections of the paper, you would find the most interesting and worthwhile to go deep on.
\end{acks}


\appendix

\section{Code}

\subsection{Generate graph}

\begin{lstlisting}[language=python]
import numpy as np
# union all edge lists into one graph

# load data
path = "data/graphs/"
edges1 = np.genfromtxt(path+"anthrocon_edgelist.csv", dtype=int, delimiter=',')
edges2 = np.genfromtxt(path+"comiccon2017_edgelist.csv", dtype=int, delimiter=',')
edges3 = np.genfromtxt(path+"icann2016_edgelist.csv", dtype=int, delimiter=',')

# union edges
graph = np.concatenate([edges1, edges2, edges3])
graph.shape

np.savetxt('twitter.csv', graph, delimiter=',', fmt=['%i', '%i'])
\end{lstlisting}

\subsection{Generate Labels}

\begin{lstlisting}[language=python]
import pandas as pd
# generate labels from tweet data

# load data
path = "data/tweets/"
tweets1 = pd.read_csv(path+"anthrocon_tweets.csv", delimiter=',')
tweets2 = pd.read_csv(path+"comiccon2017_tweets.csv", delimiter=',')
tweets3 = pd.read_csv(path+"icann2016_tweets.csv", delimiter=',')

def get_attendees(tweets):
    return tweets.loc[tweets['attendance_label'] == 1]['node_id']

nodes1 = get_attendees(tweets1).to_numpy()
nodes2 = get_attendees(tweets2).to_numpy()
nodes3 = get_attendees(tweets3).to_numpy()

all_nodes = np.concatenate([nodes1, nodes2, nodes3])

node_min = all_nodes.min()
node_max = all_nodes.max()
print("node_min: ", node_min)
print("node_max: ", node_max)

nodes = []
for i in range(node_min, node_max + 1):
    a = (i in nodes1, i in nodes2, i in nodes3)
    
    if a == (1, 1, 1): label = 7
    elif a == (0, 1, 1): label = 6
    elif a == (1, 0, 1): label = 5
    elif a == (1, 1, 0): label = 4
    elif a == (0, 0, 1): label = 3
    elif a == (0, 1, 0): label = 2
    elif a == (1, 0, 0): label = 1
    else: label = 0
    
    nodes.append([i, label])

np.savetxt('twitter.labels', nodes, delimiter='\t', fmt=['%i', '%i'])
\end{lstlisting}

\subsection{Read CSVs} \label{edge_lists}

\begin{lstlisting}[language=python]
import numpy as np
from scipy.sparse import coo_matrix

def adjacency_matrix_from_edges(edges):
    return coo_matrix((np.zeros(len(edges)), (edges[:, 0], edges[:, 1])))

def load_csv_edges(file_, undirected=True):
  edges = np.genfromtxt(file_, dtype=int, delimiter=',')

  adj_matrix = adjacency_matrix_from_edges(edges)

  return from_numpy(adj_matrix, undirected)
\end{lstlisting}


\end{document}
\endinput
